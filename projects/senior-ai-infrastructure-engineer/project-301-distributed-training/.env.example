# Ray Configuration
RAY_ADDRESS=ray://ray-head:10001
RAY_DASHBOARD_HOST=0.0.0.0
RAY_DASHBOARD_PORT=8265

# GPU Configuration
CUDA_VISIBLE_DEVICES=0,1
NCCL_DEBUG=INFO
NCCL_SOCKET_IFNAME=eth0
NCCL_IB_DISABLE=0
NCCL_P2P_LEVEL=NVL
NCCL_TIMEOUT=3600

# Training Configuration
NUM_WORKERS=4
GPUS_PER_WORKER=2
BATCH_SIZE=128
LEARNING_RATE=0.001
NUM_EPOCHS=100

# Checkpoint Configuration
CHECKPOINT_DIR=/mnt/checkpoints
CHECKPOINT_FREQUENCY=5  # Save every N epochs
CHECKPOINT_KEEP_N=5     # Keep last N checkpoints

# MLflow Configuration
MLFLOW_TRACKING_URI=http://mlflow:5000
MLFLOW_EXPERIMENT_NAME=distributed-training
MLFLOW_S3_ENDPOINT_URL=http://minio:9000
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin

# Data Configuration
DATA_DIR=/mnt/data
DATASET=imagenet
NUM_DATALOADER_WORKERS=4
PREFETCH_FACTOR=2

# Monitoring Configuration
PROMETHEUS_PUSHGATEWAY=http://prometheus-pushgateway:9091
ENABLE_PROFILING=false
PROFILING_OUTPUT_DIR=/tmp/profiling

# Model Configuration
MODEL_NAME=resnet50
MODEL_CHECKPOINT=  # Leave empty for training from scratch

# Optimization Configuration
MIXED_PRECISION=true
GRADIENT_ACCUMULATION_STEPS=1
GRADIENT_CLIPPING_NORM=1.0
ENABLE_GRADIENT_CHECKPOINTING=false

# Distributed Configuration
BACKEND=nccl
INIT_METHOD=env://
WORLD_SIZE=8
RANK=0  # Set by Ray automatically

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE=/var/log/training.log
