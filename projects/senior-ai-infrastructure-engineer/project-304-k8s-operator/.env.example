# TrainingJob Kubernetes Operator Configuration
# Copy this file to .env and customize for your environment

# ============================================================================
# Kubernetes Configuration
# ============================================================================

# Kubernetes API server (leave empty to use kubeconfig or in-cluster config)
KUBERNETES_API_SERVER=

# Kubeconfig path (optional, uses default if not set)
KUBECONFIG=~/.kube/config

# Namespace to watch (leave empty for all namespaces)
WATCH_NAMESPACE=

# Use in-cluster config (set to true when running in Kubernetes)
IN_CLUSTER=false

# ============================================================================
# Operator Configuration
# ============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# JSON logging format (true/false)
JSON_LOGGING=false

# Number of worker threads for concurrent reconciliation
WORKER_THREADS=10

# Reconciliation timeout in seconds
RECONCILIATION_TIMEOUT=300

# Resync period in seconds (full resync of all resources)
RESYNC_PERIOD=300

# ============================================================================
# CRD Configuration
# ============================================================================

# CRD group
CRD_GROUP=mlplatform.example.com

# CRD version
CRD_VERSION=v1alpha1

# CRD plural name
CRD_PLURAL=trainingjobs

# ============================================================================
# Resource Defaults
# ============================================================================

# Default CPU request
DEFAULT_CPU_REQUEST=1

# Default memory request
DEFAULT_MEMORY_REQUEST=2Gi

# Default CPU limit
DEFAULT_CPU_LIMIT=2

# Default memory limit
DEFAULT_MEMORY_LIMIT=4Gi

# Default restart policy
DEFAULT_RESTART_POLICY=OnFailure

# Default backoff limit
DEFAULT_BACKOFF_LIMIT=3

# ============================================================================
# Checkpoint Configuration
# ============================================================================

# Enable checkpointing by default
CHECKPOINT_ENABLED=true

# Default checkpoint interval (seconds)
CHECKPOINT_INTERVAL=300

# Default checkpoint path
CHECKPOINT_PATH=/checkpoints

# Default checkpoint storage backend (pvc, s3, gcs, azure)
CHECKPOINT_STORAGE=pvc

# Checkpoint retention count
CHECKPOINT_RETENTION_COUNT=5

# ============================================================================
# Monitoring Configuration
# ============================================================================

# Enable monitoring by default
MONITORING_ENABLED=true

# Metrics port
METRICS_PORT=8000

# Health check port
HEALTH_PORT=8080

# Prometheus metrics path
METRICS_PATH=/metrics

# Default training metrics port
TRAINING_METRICS_PORT=9090

# ============================================================================
# Storage Configuration
# ============================================================================

# S3 Configuration (for checkpoint storage)
S3_BUCKET=
S3_REGION=us-east-1
S3_ENDPOINT=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# GCS Configuration (for checkpoint storage)
GCS_BUCKET=
GCS_PROJECT=
GOOGLE_APPLICATION_CREDENTIALS=

# Azure Storage Configuration
AZURE_STORAGE_ACCOUNT=
AZURE_STORAGE_KEY=
AZURE_STORAGE_CONTAINER=

# ============================================================================
# Container Registry
# ============================================================================

# Default container registry
DEFAULT_REGISTRY=docker.io

# PyTorch default image
PYTORCH_DEFAULT_IMAGE=pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

# TensorFlow default image
TENSORFLOW_DEFAULT_IMAGE=tensorflow/tensorflow:2.14.0-gpu

# JAX default image
JAX_DEFAULT_IMAGE=gcr.io/deeplearning-platform-release/jax-gpu:latest

# ============================================================================
# Distributed Training
# ============================================================================

# Default master port for distributed training
MASTER_PORT=29500

# NCCL debug level (off, info, warn)
NCCL_DEBUG=info

# NCCL socket interface
NCCL_SOCKET_IFNAME=eth0

# ============================================================================
# Leader Election (for HA)
# ============================================================================

# Enable leader election
LEADER_ELECTION_ENABLED=false

# Leader election namespace
LEADER_ELECTION_NAMESPACE=kube-system

# Leader election lease duration (seconds)
LEADER_ELECTION_LEASE_DURATION=15

# Leader election renew deadline (seconds)
LEADER_ELECTION_RENEW_DEADLINE=10

# Leader election retry period (seconds)
LEADER_ELECTION_RETRY_PERIOD=2

# ============================================================================
# Advanced Configuration
# ============================================================================

# Enable admission webhooks
ADMISSION_WEBHOOKS_ENABLED=false

# Webhook port
WEBHOOK_PORT=9443

# Webhook cert directory
WEBHOOK_CERT_DIR=/tmp/k8s-webhook-server/serving-certs

# Enable finalizers for cleanup
FINALIZERS_ENABLED=true

# TTL for completed jobs (seconds, 0 to disable)
COMPLETED_JOB_TTL=86400

# Enable automatic CRD installation
AUTO_INSTALL_CRD=true

# Dry run mode (no actual changes to cluster)
DRY_RUN=false
