# Job Posting Analysis Workbook — Senior AI Infrastructure Engineer

> Sample compiled from enterprise platform teams, AI-native startups, and cloud providers (2024–2025).

## Summary

- **Sample Size**: 26 postings  
- **Regions**: SF Bay, Seattle, NYC, London, Remote (US/EU)  
- **Industries**: Cloud/AI platform, fintech, healthcare, autonomous systems

## Aggregate Skill Frequency

| Skill | Frequency | % of Postings | Category | Notes |
|-------|-----------|---------------|----------|-------|
| Advanced Kubernetes (operators, CRDs) | 24 | 92% | Core | Expect building controllers and GitOps pipelines |
| Distributed training frameworks (Ray, Horovod) | 22 | 85% | Core | Multi-GPU / multi-node orchestration |
| GPU optimization & CUDA | 21 | 81% | Core | Profiling, TensorRT, MIG/vGPU management |
| Multi-cloud architecture & networking | 20 | 77% | Core | Hybrid deployments, private connectivity |
| Infrastructure as Code at scale (Terraform/Pulumi) | 19 | 73% | Core | Module authoring, policy as code |
| Observability / SRE practices | 18 | 69% | Important | SLOs, incident leadership, chaos drills |
| Security & compliance (IAM, policy, audit) | 16 | 62% | Important | FedRAMP/SOC2 readiness, secrets governance |
| Leadership & mentorship | 15 | 58% | Important | Team technical direction, code review rigor |

## Technology Mentions

| Category | Tool / Platform | Frequency | Notes |
|----------|-----------------|-----------|-------|
| Distributed Compute | Ray, Horovod, PyTorch DDP/FSDP | 22 | Need to scale training/inference |
| GPU Stack | CUDA, Nsight, TensorRT, Triton | 21 | Performance tuning and profiling |
| Orchestration | Kubernetes, ArgoCD, FluxCD | 23 | GitOps-heavy environments |
| IaC | Terraform, Crossplane, Pulumi | 19 | Platform API + policy integration |
| Observability | Prometheus, Grafana, OpenTelemetry, Datadog | 18 | Unified telemetry for ML workloads |
| Security | OPA, Kyverno, Vault, BeyondCorp | 14 | Policy enforcement and secret automation |
| LLM Tooling | vLLM, text-generation-inference, LangChain | 13 | Optimization for generative AI workloads |

## Responsibilities Themes

| Theme | Representative Responsibilities | Frequency |
|-------|--------------------------------|-----------|
| Platform Architecture | Design multi-region ML/LLM infrastructure, evaluate tooling | 24 |
| Performance & Cost Optimization | Improve GPU efficiency, latency, throughput | 22 |
| Reliability Leadership | Lead incident response, define SLOs, run chaos drills | 19 |
| Automation & Governance | Build IaC modules, enforce policy-as-code, GitOps | 18 |
| Mentorship & Stakeholder Mgmt | Coach engineers, align with product/security leaders | 17 |

## Experience Expectations

- **Years of Experience**: 5-8+ years in ML infrastructure, SRE, or platform engineering with direct production ownership.
- **Education**: BS required; MS/PhD or equivalent domain expertise preferred but not mandatory.
- **Certifications (Preferred)**: CKA/CKS, NVIDIA DLI certificates, HashiCorp Terraform Professional, AWS/GCP Professional-level.

## Insights & Actions

- Senior curriculum must emphasise operator development, policy automation, and multi-region resiliency.
- Projects should require quantifiable performance gains (e.g., GPU utilization, latency reduction) and include governance artifacts.
- Assessments should cover architectural decision-making, incident leadership, and mentoring scenarios in addition to hands-on labs.
