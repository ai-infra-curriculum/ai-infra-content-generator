# MOD-524 Â· Transformer & LLM Optimization

Delivers advanced techniques for accelerating transformer-based and large language models across hardware and software stacks.

## Learning Goals
- Optimize attention kernels, KV cache management, and batching strategies for LLM workloads.
- Evaluate libraries/frameworks (FlashAttention, vLLM, TensorRT-LLM, DeepSpeed-Inference) and tailor them to production needs.
- Maintain accuracy and responsible AI compliance through automated evaluation harnesses.

## Legacy Source
- `learning/ai-infra-performance-learning/lessons/mod-004-transformer-optimization`

## Cross-Role Integration
- Shares evaluation and guardrail patterns with MLOps MOD-560 and architect responsible AI programs.
- Provides optimized artifacts to ML Platform developer portals and distributed inference modules.
- Collaborates with security to validate kernel changes and mitigate adversarial risk.
