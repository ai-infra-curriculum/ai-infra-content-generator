# Lecture Notes Index â€” MOD-524 Transformer & LLM Optimization

1. [Attention Optimization Techniques](01-attention-optimization.md)
2. [KV Cache & Streaming Strategies](02-kv-cache-streaming.md)
3. [Responsible LLM Performance](03-responsible-llm-performance.md)
