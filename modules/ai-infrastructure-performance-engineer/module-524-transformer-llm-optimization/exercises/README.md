# Exercises — MOD-524 Transformer & LLM Optimization

1. **Attention Optimization Lab** — Compare standard attention with FlashAttention and fused kernels, documenting performance gains.
2. **KV Cache Tuning** — Implement cache optimizations (paged KV, sliding windows) and evaluate latency/throughput trade-offs.
3. **Continuous Batching Workshop** — Configure continuous batching/streaming inference and assess SLO compliance.
