# MOD-522 Â· CUDA & Kernel Optimization

Focuses on designing, implementing, and tuning custom CUDA/Triton kernels to accelerate ML workloads.

## Learning Goals
- Profile kernels and diagnose bottlenecks using Nsight Compute and related tooling.
- Implement fused/custom kernels (CUDA, Triton) and integrate them with ML frameworks.
- Establish testing and benchmarking practices to maintain correctness and regression visibility.

## Legacy Source
- `learning/ai-infra-performance-learning/lessons/mod-002-cuda-programming`

## Cross-Role Integration
- Shares optimized kernel libraries with ML Platform developer experience modules.
- Works with security and MLOps to ensure kernel changes include guardrails and rollback plans.
- Provides inputs to architect hardware acceleration discussions in MOD-528/PROJ-524.
