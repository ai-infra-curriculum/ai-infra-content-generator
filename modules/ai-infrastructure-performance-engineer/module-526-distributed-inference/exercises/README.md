# Exercises — MOD-526 Distributed Inference & Scaling

1. **Parallelism Configuration Lab** — Deploy an inference stack (TensorRT-LLM, vLLM, DeepSpeed-Inference) with selected parallelism and benchmark improvements.
2. **Continuous Batching & Routing** — Implement continuous batching/queueing and dynamic routing; evaluate latency and throughput.
3. **Autoscaling Scenario** — Create autoscaling policies based on GPU utilization and SLO metrics; simulate load to validate behavior.
