# Lecture 01 · LLMOps Evaluation & Safety

## Objectives
- Design evaluation pipelines that measure quality, safety, and alignment for LLM applications.
- Integrate human feedback loops, guardrails, and policy checks into automated workflows.
- Coordinate evaluation outputs with governance and incident response stakeholders.

## Key Topics
1. **Evaluation Dimensions** — factual accuracy, hallucination rate, toxicity, bias, latency, cost.
2. **Tooling & Frameworks** — prompt/unit tests, red teaming, guardrails (LangChain, Guidance, Truss), human review tooling.
3. **Automation** — orchestrating evaluation in CI/CD, continuous monitoring, retraining triggers.
4. **Safety & Compliance** — responsible AI policies, legal guidelines, user privacy.
5. **Integration** — feeding results into governance evidence packs and production ops dashboards.

## Activities
- Implement an evaluation pipeline using shared architect templates.
- Configure guardrails that trigger rollback or human review on policy violations.
- Document evaluation outputs used in LLMOps reliability project PROJ-555.
