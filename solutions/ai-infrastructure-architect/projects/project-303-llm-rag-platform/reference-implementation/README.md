# Project 303: LLM Platform with RAG - Reference Implementation

This directory contains production-ready reference implementations for deploying an enterprise LLM platform with RAG capabilities.

## üìÅ Directory Structure

```
reference-implementation/
‚îú‚îÄ‚îÄ terraform/              # Infrastructure as Code
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpu-nodes/     # GPU node groups (A100, L40S)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eks/           # EKS cluster configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vpc/           # VPC and networking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector-db/     # Vector database (Qdrant)
‚îÇ   ‚îî‚îÄ‚îÄ environments/
‚îÇ       ‚îî‚îÄ‚îÄ production/    # Production environment config
‚îú‚îÄ‚îÄ kubernetes/            # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ vllm/             # vLLM deployment (Llama 3 70B, Mistral 7B)
‚îÇ   ‚îú‚îÄ‚îÄ rag-pipeline/     # RAG service deployment
‚îÇ   ‚îú‚îÄ‚îÄ vector-db/        # Qdrant deployment
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/       # Prometheus, Grafana
‚îú‚îÄ‚îÄ python/               # Python implementation
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/         # RAG pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guardrails/  # Safety guardrails
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/         # FastAPI service
‚îÇ   ‚îî‚îÄ‚îÄ tests/           # Unit and integration tests
‚îî‚îÄ‚îÄ monitoring/          # Monitoring configurations
    ‚îú‚îÄ‚îÄ prometheus/      # Prometheus rules and alerts
    ‚îî‚îÄ‚îÄ grafana/         # Grafana dashboards
```

## üöÄ Quick Start

### Prerequisites

- AWS account with GPU quotas (p4d.24xlarge, g5.12xlarge)
- Terraform >= 1.5.0
- kubectl >= 1.28.0
- Python 3.11+
- Docker

### 1. Deploy Infrastructure

```bash
cd terraform/environments/production

# Initialize Terraform
terraform init

# Plan deployment
terraform plan -out=tfplan

# Apply (creates EKS cluster, GPU nodes, VPC)
terraform apply tfplan
```

**Resources Created**:
- EKS cluster with GPU-enabled node groups
- 2x A100 nodes (p4d.24xlarge) for Llama 3 70B
- 1x L40S node (g5.12xlarge) for Mistral 7B
- VPC with public/private subnets across 3 AZs
- S3 buckets for model storage
- IAM roles with IRSA for service accounts

**Cost**: ~$75K/month (2 A100 nodes) + $25K/month (1 L40S node) = $100K/month

### 2. Deploy LLM Inference

```bash
# Configure kubectl
aws eks update-kubeconfig --name llm-platform --region us-west-2

# Create namespace
kubectl create namespace llm-inference

# Deploy vLLM for Llama 3 70B
kubectl apply -f kubernetes/vllm/llama-3-70b-deployment.yaml

# Wait for deployment
kubectl rollout status deployment/vllm-llama-3-70b -n llm-inference

# Verify
kubectl get pods -n llm-inference
```

**Expected Output**:
```
NAME                                READY   STATUS    RESTARTS   AGE
vllm-llama-3-70b-5d9c4b6f7-abcde   2/2     Running   0          5m
vllm-llama-3-70b-5d9c4b6f7-fghij   2/2     Running   0          5m
```

### 3. Deploy RAG Pipeline

```bash
# Deploy vector database (Qdrant)
kubectl apply -f kubernetes/vector-db/qdrant-deployment.yaml

# Build RAG service Docker image
cd python
docker build -t rag-service:latest -f Dockerfile .

# Push to ECR
aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-west-2.amazonaws.com
docker tag rag-service:latest <account-id>.dkr.ecr.us-west-2.amazonaws.com/rag-service:latest
docker push <account-id>.dkr.ecr.us-west-2.amazonaws.com/rag-service:latest

# Deploy RAG service
kubectl apply -f kubernetes/rag-pipeline/rag-service-deployment.yaml
```

### 4. Test the System

```bash
# Port-forward to vLLM service
kubectl port-forward svc/vllm-llama-3-70b 8000:80 -n llm-inference &

# Test inference
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3-70b",
    "messages": [{"role": "user", "content": "What is machine learning?"}],
    "temperature": 0.7,
    "max_tokens": 200
  }'

# Test RAG pipeline
cd python
python -m src.rag.pipeline
```

## üèóÔ∏è Architecture Components

### 1. GPU Infrastructure (Terraform)

**`terraform/modules/gpu-nodes/main.tf`**:
- Deploys EKS node groups with GPU instances
- A100 nodes: 8x GPUs, 96 vCPUs, 1.1TB RAM (for Llama 3 70B)
- L40S nodes: 4x GPUs, 48 vCPUs, 192GB RAM (for Mistral 7B)
- CUDA 12.2 drivers, NVIDIA container toolkit
- GPU feature discovery DaemonSet
- CloudWatch agent for GPU metrics

**Key Features**:
- Auto-scaling based on GPU utilization
- Spot instances for L40S (70% cost savings)
- On-demand for A100 (critical workload)
- 500GB EBS volumes for model storage
- IMDSv2 enforced for security

### 2. vLLM Deployment (Kubernetes)

**`kubernetes/vllm/llama-3-70b-deployment.yaml`**:
- vLLM server with PagedAttention and continuous batching
- Tensor parallelism across 8 GPUs
- Batch size: 16,384 tokens
- Max concurrent sequences: 256
- Mixed precision (bfloat16) for 2x throughput
- Horizontal Pod Autoscaler (HPA) based on GPU utilization
- PodDisruptionBudget for high availability

**Performance**:
- **Throughput**: 12,000 tokens/sec (10x vs naive)
- **Latency**: P95 < 650ms (meets <800ms SLO)
- **Capacity**: 10,000+ concurrent users

### 3. RAG Pipeline (Python)

**`python/src/rag/pipeline.py`**:
- Two-stage retrieval architecture
- **Stage 1**: Dense retrieval using vector embeddings (Qdrant)
- **Stage 2**: Reranking with cross-encoder for accuracy
- **Stage 3**: LLM generation with retrieved context

**Components**:
- Embedding model: BAAI/bge-large-en-v1.5 (1024 dims)
- Reranker: BAAI/bge-reranker-large
- Vector DB: Qdrant with HNSW index
- Context window: 3000 tokens (reserves for generation)

**Performance**:
- End-to-end latency: <1.5s (P95)
- Retrieval: <200ms for 100 documents
- Reranking: <100ms for 10 documents
- Generation: <1s for 200 tokens

### 4. Safety Guardrails (Python)

**`python/src/guardrails/safety.py`**:
- Multi-layered safety system
- **Input validation**:
  - PII detection and redaction (Presidio)
  - Prompt injection detection (regex patterns)
  - Content moderation (toxic-bert)
  - Rate limiting (60 req/min per user)
- **Output validation**:
  - Toxicity filtering
  - Length limits
  - Format validation

**PII Detection**:
- Detects: SSN, credit cards, emails, phone numbers, addresses
- Risk levels: CRITICAL (SSN, credit card) ‚Üí HIGH (email, phone) ‚Üí MEDIUM ‚Üí LOW
- Automatic redaction with anonymization

**Prompt Injection Patterns**:
- "Ignore previous instructions"
- "Act as a different role"
- "Show me your system prompt"
- SQL injection style attacks

### 5. Monitoring (Prometheus)

**`monitoring/prometheus/llm-rules.yaml`**:
- **Performance metrics**:
  - P95/P99 latency tracking
  - Throughput (requests/sec, tokens/sec)
  - GPU utilization and memory
- **Alerts**:
  - High latency (warning >2s, critical >5s)
  - Low throughput (<10 req/s)
  - High error rate (>5%)
  - GPU utilization (>95% for 15min)
  - Safety violations
  - Cost overruns (>$100/hour)

**Grafana Dashboards** (in `monitoring/grafana/`):
- LLM performance dashboard
- GPU metrics dashboard
- RAG pipeline metrics
- Cost optimization dashboard

## üí∞ Cost Optimization

### Current Costs
- **A100 nodes**: $75K/month (2 nodes √ó $37.5K)
- **L40S nodes**: $25K/month (1 node √ó $25K, Spot 70% discount)
- **Infrastructure**: $10K/month (EKS, S3, networking)
- **Total**: $110K/month

### Optimization Strategies (from ADR-009)
1. **Spot instances for L40S**: -70% cost ($75K ‚Üí $25K)
2. **GPU utilization**: 35% ‚Üí 70% (2x effective capacity)
3. **Batch optimization**: 10x throughput with continuous batching
4. **Model routing**: 70% traffic to self-hosted (vs 100% commercial APIs)
5. **Savings**: $500K/month (baseline) ‚Üí $150K/month (70% reduction)

## üîí Security

### Data Privacy
- Sensitive data never leaves infrastructure (self-hosted models)
- PII detection and redaction for all inputs
- Encryption at rest (EBS, S3 with KMS)
- Encryption in transit (TLS 1.3)

### Compliance
- SOC2 Type II ready
- HIPAA-compliant deployment option (dedicated nodes)
- GDPR-compliant data handling
- Audit logging (CloudWatch, S3)

### Access Control
- IRSA (IAM Roles for Service Accounts) for pod-level permissions
- Network policies for namespace isolation
- RBAC for Kubernetes resources
- VPC isolation with private subnets

## üìä Monitoring and Observability

### Metrics Collection
- **Prometheus**: Scrapes vLLM, DCGM (GPU), application metrics
- **CloudWatch**: AWS infrastructure metrics
- **Grafana**: Visualization dashboards

### Log Aggregation
- **FluentBit**: Collects container logs
- **CloudWatch Logs**: Centralized log storage
- **Elasticsearch** (optional): Advanced log analysis

### Tracing
- **OpenTelemetry**: Distributed tracing for RAG pipeline
- **Jaeger**: Trace visualization

## üß™ Testing

```bash
cd python

# Install dependencies
pip install -r requirements.txt -r requirements-dev.txt

# Unit tests
pytest tests/unit/ -v

# Integration tests (requires running services)
pytest tests/integration/ -v

# Load tests
locust -f tests/load/locustfile.py --host=http://localhost:8000
```

## üìö Additional Resources

- [../ARCHITECTURE.md](../ARCHITECTURE.md) - Comprehensive architecture documentation
- [../architecture/decisions/](../architecture/decisions/) - Architecture Decision Records (ADRs)
- [../business/business-case.md](../business/business-case.md) - Business justification ($64.6M NPV)
- [../governance/llm-governance-framework.md](../governance/llm-governance-framework.md) - Governance policies
- [../runbooks/](../runbooks/) - Operational runbooks

## ü§ù Contributing

1. Follow the code style (black, isort, mypy)
2. Add tests for new features
3. Update documentation
4. Submit PR with clear description

## üìù License

Internal use only - Company Confidential

## üêõ Troubleshooting

See [../runbooks/troubleshooting-guide.md](../runbooks/troubleshooting-guide.md) for common issues and solutions.

## üìß Support

- Slack: #llm-platform
- Email: llm-platform@company.com
- Oncall: PagerDuty rotation

---

**Last Updated**: 2025-01-15
**Version**: 1.0
**Maintained By**: AI Infrastructure Team
