# Enterprise MLOps Platform - Technical Deep Dive

## Presentation Metadata
- **Audience**: Engineering teams (Data Scientists, ML Engineers, SREs, Security)
- **Duration**: 60 minutes + 30 minutes Q&A
- **Format**: Technical workshop with live demos
- **Objective**: Build technical buy-in and prepare teams for adoption

---

## Slide 1: Technical Overview

### What We're Building: A Complete MLOps Platform

**Platform Components**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Science Workbench                         â”‚
â”‚  Jupyter, VS Code, Python, Git                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Platform Services Layer                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   MLflow     â”‚    Feast     â”‚   KServe     â”‚   Platform API    â”‚
â”‚  (Registry)  â”‚ (Features)   â”‚  (Serving)   â”‚  (Orchestration)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Layer (Kubernetes on EKS)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Compute Nodes  â”‚   GPU Nodes     â”‚   Storage (S3)   â”‚   RDS   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Principles**:
1. **Cloud-Native**: Kubernetes, containers, microservices
2. **Self-Service**: Data scientists deploy without IT tickets
3. **Production-Grade**: HA, auto-scaling, monitoring built-in
4. **Cost-Optimized**: Spot instances, auto-scaling, resource quotas
5. **Secure by Default**: Encryption, RBAC, network policies

---

## Slide 2: Architecture Decision Records (ADRs)

### 10 Key Decisions That Shape the Platform

| ADR | Decision | Why | Alternatives Rejected |
|-----|----------|-----|----------------------|
| 001 | AWS + EKS + Terraform | Industry standard, mature ecosystem | GCP (team expertise), Azure (cost) |
| 002 | Feast for feature store | Open-source, flexible, S3/Redshift integration | Tecton (cost: $2.1M more) |
| 003 | Namespace-based multi-tenancy | Native K8s, simple, effective | Service mesh (complexity), separate clusters (cost) |
| 004 | S3 data lake + Redshift warehouse | Cost-effective, scalable | Databricks ($850K/year more) |
| 005 | MLflow for model registry | Open-source, widely adopted, extensible | SageMaker (vendor lock-in), custom (build cost) |
| 006 | Flink on K8s for streaming | Exactly-once semantics, <1s latency | Spark Streaming (latency), Kafka Streams (operational complexity) |
| 007 | Defense-in-depth security | Multiple layers, compliance-ready | Perimeter security only (insufficient for HIPAA) |
| 008 | Amazon EKS managed service | $600K/year savings vs. self-managed | Self-managed K8s (ops burden), ECS (less flexible) |
| 009 | Spot instances + auto-scaling | 70% cost savings on compute | On-demand only (expensive), Reserved Instances (less flexible) |
| 010 | Risk-based governance | Balance speed and safety | Manual approval (slow), no governance (risky) |

**Full ADRs Available**: `/architecture/adrs/*.md` (10 documents, 25,000 words)

---

## Slide 3: Technology Stack

### Our Choices and Why

**Infrastructure**:
- â˜ï¸ **AWS**: Industry standard, team expertise, regulatory compliance support
- ğŸ›ï¸ **Terraform**: Infrastructure as Code, reproducible, version-controlled
- â˜¸ï¸ **Kubernetes (EKS)**: Container orchestration, auto-scaling, self-healing

**ML Lifecycle**:
- ğŸ“Š **MLflow** (2.8.0): Experiment tracking, model registry, artifact storage
- ğŸ± **Feast** (latest): Feature store, online/offline serving
- ğŸš€ **KServe** (0.11): Model serving, auto-scaling, canary deployments
- ğŸ“ˆ **Prometheus + Grafana**: Monitoring, alerting, dashboards

**Data Platform**:
- ğŸ—„ï¸ **S3**: Data lake (raw, processed, features)
- ğŸ” **Redshift**: Data warehouse (analytics, batch features)
- âš¡ **Redis**: Real-time feature cache (<10ms latency)
- ğŸ˜ **PostgreSQL**: Metadata (MLflow, Feast registry)
- ğŸŒŠ **Apache Flink**: Stream processing (real-time features)

**Development**:
- ğŸ **Python 3.11**: Primary language (ML, APIs, automation)
- âš™ï¸ **FastAPI**: Platform API (high performance, async)
- ğŸ³ **Docker**: Containerization
- ğŸ“¦ **Helm**: Kubernetes package manager

---

## Slide 4: Network Architecture

### Multi-AZ, Highly Available, Secure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VPC (10.0.0.0/16) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Availability Zone 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€ Public Subnet (10.0.0.0/20) â”€â”€â”€â”                    â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ NAT Gateway                      â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Load Balancers                   â”‚                    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚  â”‚                                                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€ Private Subnet (10.0.16.0/20) â”€â”€â”                   â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ EKS Worker Nodes                 â”‚                    â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Application Pods                 â”‚                    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚  â”‚                                                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€ Database Subnet (10.0.32.0/20) â”€â”                   â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ RDS PostgreSQL (isolated)        â”‚                    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Availability Zone 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  [Same structure as AZ1]                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Availability Zone 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  [Same structure as AZ1]                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  Security Features:                                              â”‚
â”‚  â€¢ VPC Flow Logs â†’ CloudWatch Logs (7-year retention)           â”‚
â”‚  â€¢ Security Groups (stateful firewall)                           â”‚
â”‚  â€¢ Network ACLs (stateless firewall)                             â”‚
â”‚  â€¢ VPC Endpoints (S3, DynamoDB, ECR) - no internet egress       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions**:
- **3 AZs**: Survive zone failures
- **3 subnet types**: Isolation (public, private, database)
- **NAT Gateways**: One per AZ (high availability)
- **VPC Endpoints**: Cost savings + security ($20K/year savings)

---

## Slide 5: Kubernetes Architecture

### EKS Cluster Design

**Control Plane** (AWS Managed):
- Kubernetes 1.27+
- Multi-AZ API servers (HA)
- Encrypted etcd (envelope encryption with KMS)
- Audit logging enabled

**Node Groups** (3 types):

1. **System Nodes** (On-Demand, m5.2xlarge):
   - Purpose: Platform services (MLflow, Feast, monitoring)
   - Min: 3, Max: 5, Desired: 3
   - Labels: `workload-type=system`
   - No spot instances (need reliability)

2. **Compute Nodes** (Spot, m5.4xlarge):
   - Purpose: ML training, batch inference
   - Min: 0, Max: 20, Desired: 5
   - Labels: `workload-type=compute`
   - 70% cost savings with spot

3. **GPU Nodes** (Spot, g4dn.2xlarge):
   - Purpose: GPU-accelerated training
   - Min: 0, Max: 10, Desired: 0 (scale from zero)
   - Labels: `workload-type=gpu`
   - Taints: `nvidia.com/gpu=true:NoSchedule`

**Add-ons**:
- VPC CNI (networking)
- CoreDNS (service discovery)
- kube-proxy (networking)
- EBS CSI Driver (persistent volumes)
- Cluster Autoscaler (node scaling)
- Metrics Server (HPA support)

---

## Slide 6: Storage Architecture

### Multi-Tier Storage Strategy

**S3 Data Lake** (Primary Storage):
```
s3://mlops-data-lake/
â”œâ”€â”€ raw/                    # Landing zone
â”‚   â”œâ”€â”€ events/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ external/
â”œâ”€â”€ processed/              # Cleaned, validated
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ training-data/
â”‚   â””â”€â”€ inference-data/
â”œâ”€â”€ models/                 # Model artifacts
â”‚   â”œâ”€â”€ {model-name}/
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ v2/
â”‚   â”‚   â””â”€â”€ latest/
â””â”€â”€ artifacts/              # MLflow artifacts
    â”œâ”€â”€ experiments/
    â””â”€â”€ runs/
```

**Lifecycle Policies**:
- 0-90 days: S3 Standard (hot)
- 91-365 days: S3 Intelligent-Tiering (warm)
- 366+ days: S3 Glacier (cold, $1/TB/month)

**Redshift Data Warehouse**:
- Use case: Batch features, analytics, dashboards
- Cluster: 3 nodes (ra3.xlplus), multi-AZ
- Concurrency: 50 concurrent queries
- Cost: ~$10K/month

**Redis Cache**:
- Use case: Real-time feature serving (<10ms)
- Cluster: ElastiCache, 3 nodes, multi-AZ
- Size: 50 GB in-memory
- Cost: ~$2K/month

**RDS PostgreSQL**:
- Use case: Metadata (MLflow, Feast, Platform API)
- Instance: db.r5.xlarge, multi-AZ
- Storage: 500 GB GP3
- Automated backups (30-day retention)
- Cost: ~$500/month

**Total Storage Cost**: ~$13K/month + S3 usage (~$5K/month)

---

## Slide 7: MLflow Architecture

### Experiment Tracking & Model Registry

**Components**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Scientists                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ mlflow.log_model()
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   MLflow Tracking     â”‚
                â”‚      Server           â”‚
                â”‚   (3 replicas, HA)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                     â”‚
              â–¼                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PostgreSQL     â”‚   â”‚   S3 Bucket     â”‚
    â”‚  (Metadata)     â”‚   â”‚  (Artifacts)    â”‚
    â”‚  â€¢ Runs         â”‚   â”‚  â€¢ Models       â”‚
    â”‚  â€¢ Experiments  â”‚   â”‚  â€¢ Metrics      â”‚
    â”‚  â€¢ Models       â”‚   â”‚  â€¢ Plots        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:
- **Experiment Tracking**: Log params, metrics, artifacts
- **Model Registry**: Version models, stage transitions (Dev â†’ Staging â†’ Prod)
- **Model Lineage**: Track data, code, parameters for reproducibility
- **Artifact Storage**: Models, plots, data samples in S3
- **Search & Compare**: Query experiments, compare runs

**Deployment**:
- Kubernetes Deployment (3 replicas)
- HPA: Scale 3-10 based on CPU/memory
- Ingress: TLS, authentication (OAuth2 Proxy)
- Monitoring: Prometheus scraping `/metrics`

**Performance**:
- P95 latency: 800ms
- Throughput: 100 req/sec
- Concurrent users: 50+

---

## Slide 8: Feature Store (Feast)

### Solving the Feature Problem

**Problem**: Features are inconsistent between training and serving
- Training uses batch data (BigQuery, Redshift)
- Serving needs real-time (<100ms latency)
- Feature engineering logic duplicated
- Training-serving skew leads to poor model performance

**Solution: Feast Feature Store**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Feature Definitions                      â”‚
â”‚  â€¢ customer_features  â€¢ product_features  â€¢ event_features   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Offline Store   â”‚     â”‚  Online Store    â”‚
â”‚  (Redshift)     â”‚     â”‚    (Redis)       â”‚
â”‚                 â”‚     â”‚                  â”‚
â”‚ â€¢ Training      â”‚     â”‚ â€¢ Serving        â”‚
â”‚ â€¢ Backfills     â”‚     â”‚ â€¢ <10ms latency  â”‚
â”‚ â€¢ Analytics     â”‚     â”‚ â€¢ Point lookup   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Feature Definition** (Python):
```python
from feast import Entity, Feature, FeatureView, Field
from feast.types import Float32, Int64

# Define entity
customer = Entity(
    name="customer_id",
    join_keys=["customer_id"],
    description="Customer entity"
)

# Define feature view
customer_features = FeatureView(
    name="customer_features",
    entities=[customer],
    schema=[
        Field(name="total_purchases", dtype=Int64),
        Field(name="avg_order_value", dtype=Float32),
        Field(name="days_since_last_purchase", dtype=Int64)
    ],
    source=... # Redshift table
)
```

**Feature Retrieval**:
```python
# Training (batch)
training_df = store.get_historical_features(
    entity_df=entity_df,
    features=["customer_features:total_purchases", ...]
).to_df()

# Serving (online)
features = store.get_online_features(
    features=["customer_features:total_purchases", ...],
    entity_rows=[{"customer_id": "123"}]
).to_dict()
```

**Benefits**:
- **Consistency**: Same features for training and serving
- **Reusability**: Share features across models and teams
- **Performance**: <10ms online serving latency
- **Time Travel**: Historical feature values for training

---

## Slide 9: Model Serving (KServe)

### Production Model Deployment

**KServe InferenceService**:
```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: churn-predictor
spec:
  predictor:
    minReplicas: 2
    maxReplicas: 10
    model:
      modelFormat:
        name: mlflow
      storageUri: s3://mlops-models/churn-predictor/v1
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"
        limits:
          cpu: "4"
          memory: "8Gi"
```

**Features**:
- **Auto-Scaling**: HPA based on requests/concurrency
- **Canary Deployments**: Route % traffic to new version
- **Multi-Model Serving**: Host multiple models per pod
- **GPU Support**: Automatic GPU allocation
- **Metrics**: Prometheus metrics out-of-the-box

**Request Flow**:
```
Client
  â”‚
  â–¼
Ingress (NGINX)
  â”‚
  â–¼
KServe Predictor Service
  â”‚
  â”œâ”€ v1 (90% traffic)
  â””â”€ v2 (10% traffic) â† Canary
       â”‚
       â–¼
   Model Pod
    â”œâ”€ Transformer (preprocessing)
    â”œâ”€ Predictor (inference)
    â””â”€ Explainer (SHAP values)
```

**Performance**:
- P95 latency: 120ms
- Throughput: 500 req/sec per model
- Concurrent models: 50+
- GPU utilization: 70% (optimized)

---

## Slide 10: Monitoring & Observability

### 3 Pillars: Metrics, Logs, Traces

**Metrics (Prometheus)**:
```
# Infrastructure metrics
node_cpu_usage, node_memory_usage, disk_io

# Application metrics
http_requests_total, http_request_duration_seconds

# ML metrics
model_predictions_total, model_latency_seconds, model_error_rate

# Business metrics
daily_active_models, cost_per_prediction, data_quality_score
```

**Logs (CloudWatch Logs + Fluentd)**:
```
2025-10-17T14:22:33Z INFO [model=churn-predictor v=2]
  Prediction: customer_id=123, churn_probability=0.72, latency=95ms
```

**Traces (Jaeger)** [Future]:
- Distributed tracing for request flows
- Identify bottlenecks in prediction pipeline
- Correlate errors across services

**Dashboards (Grafana)**:
- Platform Overview (health, usage, cost)
- Model Performance (latency, errors, throughput)
- Infrastructure (CPU, memory, GPU utilization)
- Cost Analysis (by service, by team, trends)

**Alerting**:
- PagerDuty for critical alerts
- Slack for warnings
- Email for informational

---

## Slide 11: Security Architecture

### Defense-in-Depth: 5 Layers

**Layer 1: Network Security**
- VPC isolation, Security Groups, NACLs
- No public internet access for private subnets
- VPC endpoints for AWS services (no internet)
- WAF (future) for ingress protection

**Layer 2: Identity & Access**
- AWS IAM for infrastructure access
- Kubernetes RBAC for cluster access
- IRSA (IAM Roles for Service Accounts)
- MFA required for all human access
- SSO via Okta (future)

**Layer 3: Data Protection**
- Encryption at rest: AES-256 (S3, EBS, RDS)
- Encryption in transit: TLS 1.3
- KMS key rotation (annual)
- Secrets management: AWS Secrets Manager
- No secrets in code or containers

**Layer 4: Monitoring & Detection**
- VPC Flow Logs (all traffic logged)
- CloudTrail (API calls logged)
- Kubernetes audit logs
- GuardDuty (threat detection)
- SIEM integration (Splunk)

**Layer 5: Compliance & Governance**
- SOC 2 Type II controls
- HIPAA technical safeguards
- GDPR privacy by design
- Automated compliance checks (OPA)
- Regular pen testing

**HIPAA-Specific Controls**:
- Dedicated node pool for PHI workloads
- FIPS 140-2 encryption
- Enhanced audit logging
- 15-minute session timeout
- No shared storage with non-PHI data

---

## Slide 12: Governance Framework

### Risk-Based Model Approval

**Risk Classification**:
```python
def classify_model_risk(model):
    score = 0
    score += assess_business_impact(model)     # 0-10
    score += assess_customer_exposure(model)   # 0-10
    score += assess_data_sensitivity(model)    # 0-10
    score += assess_regulatory_scope(model)    # 0-10
    score += assess_explainability_need(model) # 0-10

    if score < 15: return "low"
    if score < 30: return "medium"
    return "high"
```

**Approval Workflows**:

| Risk Level | Approvers | SLA | Auto-Deploy | Monitoring |
|------------|-----------|-----|-------------|------------|
| **Low** | Team Lead (auto) | 24 hours | âœ… Yes | Monthly review |
| **Medium** | Manager + Senior DS | 48 hours | âŒ No | Bi-weekly review |
| **High** | CAB (5+ stakeholders) | 5 days | âŒ No | Weekly review + monthly revalidation |

**Change Advisory Board (CAB)** for High-Risk Models:
- Chair: VP Engineering
- Members: ML Architect, Senior DS, Product Manager, Legal, Security
- Meeting: Weekly (Thursdays 2-4 PM)
- Quorum: 4 members including chair

**Automated Governance**:
```python
# Example: Prevent high-risk models from deploying without approval
@app.post("/api/v1/models/deploy")
async def deploy_model(request: ModelDeploymentRequest):
    risk_level = get_model_risk(request.model_name)

    if risk_level == "high" and not has_cab_approval(request):
        return {
            "status": "rejected",
            "message": "High-risk models require CAB approval"
        }

    # Proceed with deployment
    ...
```

**Audit Trail**:
- All actions logged to DynamoDB
- 7-year retention (compliance)
- Immutable logs (WORM storage)
- Full lineage (data â†’ model â†’ predictions)

---

## Slide 13: Cost Optimization

### FinOps Strategy: $1.5M/Year Savings

**Compute Optimization (70% of cost)**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strategy                    â”‚ Savings  â”‚ Risk        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Spot Instances (compute)    â”‚ 70%      â”‚ Low*        â”‚
â”‚ Spot Instances (GPU)        â”‚ 70%      â”‚ Medium*     â”‚
â”‚ Auto-scaling (0 â†’ N)        â”‚ 30%      â”‚ None        â”‚
â”‚ Right-sizing (M5 â†’ M5a)     â”‚ 10%      â”‚ None        â”‚
â”‚ Reserved Instances (system) â”‚ 40%      â”‚ None        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

*Spot interruption rate: <5% with proper handling
```

**Spot Instance Handling**:
```yaml
nodeSelector:
  capacity: SPOT

tolerations:
- key: "spot-instance"
  operator: "Equal"
  value: "true"
  effect: "NoSchedule"

# Graceful shutdown on spot termination
preStop:
  exec:
    command: ["/bin/sh", "-c", "sleep 120"]
```

**Storage Optimization (15% of cost)**:
- S3 Intelligent-Tiering (auto-move to cheaper tiers)
- Glacier for archival ($1/TB/month vs. $23/TB/month)
- Delete old artifacts (90-day retention default)
- Compress data (Parquet vs. CSV: 10x smaller)

**Monitoring Optimization**:
- Metrics downsampling (30s â†’ 5m for old data)
- Log filtering (drop debug logs after 7 days)
- Dashboard lazy loading

**Resource Quotas** (prevent waste):
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-data-science
spec:
  hard:
    requests.cpu: "100"           # $7K/month
    requests.memory: "200Gi"      # $2K/month
    requests.nvidia.com/gpu: "4"  # $10K/month
```

**Cost Allocation**:
- Tag all resources with team/cost-center
- Chargeback model: Teams pay for their usage
- Monthly cost reports per team
- Gamification: Leaderboard for cost efficiency

---

## Slide 14: CI/CD Pipeline

### Automated Model Deployment

**Pipeline Stages** (9 stages, 15-20 minutes):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Validate   â”‚ â† MLflow check, model exists
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Security   â”‚ â† Scan for vulnerabilities, compliance check
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Test       â”‚ â† Performance test, accuracy validation
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Approval   â”‚ â† Manual gate (if high-risk/production)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build      â”‚ â† Generate KServe manifest
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deploy     â”‚ â† Apply to Kubernetes
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Smoke Test â”‚ â† Verify endpoint, make test prediction
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audit      â”‚ â† Log to audit trail
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Notify     â”‚ â† Slack message, update status page
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**GitOps Workflow**:
```bash
# 1. Data scientist commits model to MLflow
mlflow.log_model(model, "model")

# 2. Create deployment PR
git checkout -b deploy-churn-v2
# Edit deployment.yaml with new version
git commit -m "Deploy churn-predictor v2"
git push && gh pr create

# 3. CI runs tests automatically
# 4. Approvers review (if needed)
# 5. Merge â†’ Automatic deployment
# 6. Monitor in Grafana
```

**Rollback Strategy**:
```bash
# Automatic rollback if:
- Error rate > 5% for 5 minutes
- P95 latency > 2x baseline
- Availability < 99%

# Manual rollback:
kubectl rollout undo deployment/churn-predictor -n models
```

---

## Slide 15: Disaster Recovery

### Business Continuity Planning

**RPO/RTO Targets**:
- **RPO (Recovery Point Objective)**: 1 hour
- **RTO (Recovery Time Objective)**: 4 hours

**Backup Strategy**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component      â”‚ Backup â”‚ Retention â”‚ RPO    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RDS PostgreSQL â”‚ Daily  â”‚ 30 days   â”‚ 24h    â”‚
â”‚ S3 Data Lake   â”‚ Versioning + Cross-region â”‚ N/A â”‚
â”‚ EKS etcd       â”‚ Velero â”‚ 30 days   â”‚ 6h     â”‚
â”‚ Redis Cache    â”‚ None*  â”‚ -         â”‚ -      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

*Redis is cache-only, rebuilt from source on failure
```

**Disaster Scenarios**:

**1. AZ Failure**:
- Impact: None (multi-AZ architecture)
- Recovery: Automatic (Kubernetes reschedules pods)
- Time: 2-5 minutes

**2. Region Failure** (Catastrophic):
- Impact: Platform offline
- Recovery: Restore from backups to secondary region
- Time: 4-6 hours
- Steps:
  1. Failover DNS to us-west-2
  2. Restore RDS from snapshot (1 hour)
  3. Terraform apply secondary region (2 hours)
  4. Deploy applications (1 hour)
  5. Validate and test (1 hour)

**3. Data Corruption**:
- Impact: Model predictions may be incorrect
- Recovery: Restore S3 objects from versioning
- Time: 30 minutes - 2 hours

**DR Testing**:
- Quarterly DR drills
- Document results and improve procedures
- Runbook: `/docs/runbooks/disaster-recovery.md`

---

## Slide 16: Performance Benchmarks

### What to Expect

**Model Serving**:
- P50 latency: 80ms
- P95 latency: 120ms
- P99 latency: 200ms
- Throughput: 500 req/sec per model
- Concurrent models: 50+

**MLflow**:
- Experiment logging: 50-100ms per operation
- Model registration: 2-5 seconds
- Artifact upload: 10-30 seconds (depends on size)
- Search queries: 100-500ms

**Feature Retrieval**:
- Online (Redis): <10ms (P95)
- Offline (Redshift): 2-5 seconds (batch)

**Infrastructure**:
- Pod startup time: 30-60 seconds
- Node provisioning: 90-120 seconds
- Cluster autoscaling: 2-3 minutes

**Throughput Limits**:
- API Gateway: 10,000 req/sec
- Ingress Controller: 50,000 req/sec
- Kubernetes: 5,000 nodes (theoretical max)

---

## Slide 17: Scaling Characteristics

### How the Platform Scales

**Horizontal Scaling** (add more pods/nodes):
- Models: Scale 1 â†’ 100+ pods per model
- Platform services: Scale 3 â†’ 10+ replicas
- Nodes: Scale 3 â†’ 100+ nodes
- Auto-scaling: HPA (pod-level), Cluster Autoscaler (node-level)

**Vertical Scaling** (bigger instances):
- Nodes: m5.large â†’ m5.24xlarge
- GPU: g4dn.xlarge â†’ p4d.24xlarge
- RDS: db.r5.large â†’ db.r5.24xlarge

**Data Scaling**:
- S3: Unlimited storage
- Redshift: 1 TB â†’ 100+ TB (add nodes)
- Redis: 50 GB â†’ 500 GB (bigger instance)

**Expected Growth**:
- Year 1: 10-20 models, 20 data scientists
- Year 2: 50-100 models, 100 data scientists
- Year 3: 100-200 models, 200 data scientists

**Capacity Planning**:
- Monthly reviews of utilization
- Predictive scaling (based on historical patterns)
- Budget alerts (CloudWatch + Lambda)

---

## Slide 18: Getting Started (For Data Scientists)

### Your First Model Deployment (20 Minutes)

**Step 1: Train and Register Model**
```python
import mlflow

mlflow.set_tracking_uri("https://mlflow.mlops-platform.com")
mlflow.set_experiment("my-experiment")

with mlflow.start_run():
    # Train your model
    model = train_my_model(X_train, y_train)

    # Log params and metrics
    mlflow.log_params({"alpha": 0.5, "l1_ratio": 0.1})
    mlflow.log_metrics({"rmse": 0.79, "r2": 0.85})

    # Log model
    mlflow.sklearn.log_model(model, "model")

    # Register model
    mlflow.register_model(
        model_uri=f"runs:/{mlflow.active_run().info.run_id}/model",
        name="my-model"
    )
```

**Step 2: Deploy via Platform API**
```python
import requests

response = requests.post(
    "https://api.mlops-platform.com/api/v1/models/deploy",
    json={
        "model_name": "my-model",
        "model_version": "1",
        "target_stage": "staging",
        "replicas": 2,
        "justification": "Initial deployment for testing"
    },
    headers={"Authorization": f"Bearer {token}"}
)

print(response.json())
# {"status": "success", "deployment_id": "abc-123"}
```

**Step 3: Make Predictions**
```python
endpoint = "https://models.mlops-platform.com/v1/models/my-model:predict"

response = requests.post(
    endpoint,
    json={"instances": [[1.0, 2.0, 3.0]]},
    headers={"Authorization": f"Bearer {token}"}
)

predictions = response.json()["predictions"]
print(predictions)  # [0.87]
```

**Step 4: Monitor Performance**
- View dashboards: `https://grafana.mlops-platform.com`
- Check alerts: `#ml-alerts` Slack channel
- Review metrics: Prometheus queries

---

## Slide 19: Migration Strategy

### Moving Existing Models to Platform

**4-Phase Migration**:

**Phase 1: Assessment (Week 1-2)**
- Inventory all existing models
- Classify by complexity/risk
- Prioritize migration order

**Phase 2: Pilot (Week 3-8)**
- Migrate 2-3 low-risk models
- Document lessons learned
- Refine procedures

**Phase 3: Bulk Migration (Week 9-20)**
- Migrate remaining models
- Dedicated migration support team
- Weekly office hours

**Phase 4: Decommission Legacy (Week 21-26)**
- Shut down old infrastructure
- Archive documentation
- Realize cost savings

**Migration Runbook**:
1. Export model from old system
2. Convert to MLflow format (if needed)
3. Re-register in MLflow
4. Test locally
5. Deploy to staging
6. Validate (compare predictions)
7. Deploy to production (canary)
8. Monitor for 1 week
9. Full cutover
10. Decommission old deployment

**Support**:
- Migration office hours: Tue/Thu 2-4 PM
- Slack: #platform-migration
- Dedicated migration engineer

---

## Slide 20: Q&A and Resources

### Common Questions

**Q: Can I use my own Docker images?**
A: Yes! As long as they expose the right API (KServe protocol). We also provide base images.

**Q: How do I access GPUs?**
A: Request GPU in your pod spec: `resources.limits.nvidia.com/gpu: 1`. Auto-scheduled to GPU nodes.

**Q: What if my model doesn't fit in memory?**
A: Use larger instance types (m5.12xlarge has 192 GB RAM) or model sharding.

**Q: How do I A/B test models?**
A: Use KServe canary deployments. Example: 90% traffic to v1, 10% to v2.

**Q: Can I run Spark jobs?**
A: Yes! Use Spark on Kubernetes operator. Example: `/docs/examples/spark-job.yaml`

**Q: How much does it cost to run my model?**
A: Typical model: ~$2K/month (3 replicas of m5.xlarge). View cost dashboard for estimates.

### Resources

**Documentation**:
- Architecture: `/ARCHITECTURE.md`
- ADRs: `/architecture/adrs/`
- Governance: `/governance/`
- Runbooks: `/docs/runbooks/`

**Code**:
- Terraform: `/terraform/`
- Kubernetes: `/kubernetes/`
- Platform API: `/platform-api/`
- Examples: `/examples/`

**Support**:
- Slack: #mlops-platform
- Email: mlops-support@company.com
- Office Hours: Tue/Thu 10-11 AM
- On-call (production): PagerDuty

### Next Steps for Your Team
1. Complete platform onboarding (1-hour training)
2. Migrate 1 pilot model
3. Provide feedback
4. Plan migration of remaining models

---

**End of Technical Presentation**

## Appendix: Live Demo Script

### Demo 1: Deploy a Model (10 minutes)

```bash
# 1. Clone example repository
git clone https://github.com/company/mlops-examples
cd mlops-examples/sklearn-model

# 2. Train and register model
python train.py

# 3. Deploy via CLI
mlops deploy \
  --model sklearn-demo \
  --version 1 \
  --environment staging \
  --replicas 2

# 4. Test prediction
curl -X POST https://models-staging.mlops-platform.com/v1/models/sklearn-demo:predict \
  -H "Content-Type: application/json" \
  -d '{"instances": [[1.0, 2.0, 3.0]]}'

# 5. View in Grafana
open https://grafana.mlops-platform.com/d/models/model-performance?var-model=sklearn-demo
```

### Demo 2: Feature Store (5 minutes)

```python
from feast import FeatureStore

store = FeatureStore(repo_path=".")

# Get online features
features = store.get_online_features(
    features=["customer_features:total_purchases"],
    entity_rows=[{"customer_id": "123"}]
).to_dict()

print(features)
# {'customer_id': ['123'], 'total_purchases': [42]}
```

### Demo 3: Monitoring (5 minutes)

1. Open Grafana: `https://grafana.mlops-platform.com`
2. Navigate to "MLOps Platform Overview" dashboard
3. Show: Model request rate, latency, error rate
4. Show: Cost tracking
5. Show: Alert firing (simulate high error rate)

---

**Document Control**

**Version**: 1.0
**Last Updated**: 2025-10-17
**Author**: AI Infrastructure Architecture Team
**Next Review**: Before technical workshops
